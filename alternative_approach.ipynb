{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Maximum Likelihood Estimation of Custom Models in Python with StatsModels\n",
    "tags: Statistics, Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimation is a common method for fitting statistical models.  In Python, it is quite possible to fit maximum likelihood models using just [`scipy.optimize`](http://docs.scipy.org/doc/scipy/reference/optimize.html). Over time, however, I have come to prefer the convenience provided by [`statsmodels`'](http://statsmodels.sourceforge.net/) [`GenericLikelihoodModel`](http://statsmodels.sourceforge.net/devel/dev/generated/statsmodels.base.model.GenericLikelihoodModel.html).  In this post, I will show how easy it is to subclass `GenericLikelihoodModel` and take advantage of much of `statsmodels`' well-developed machinery for maximum likelihood estimation of custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-inflated Poisson models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we use for this demonstration is a [zero-inflated Poisson model](http://en.wikipedia.org/wiki/Zero-inflated_model#Zero-inflated_Poisson).  This is a model for count data that generalizes the Poisson model by allowing for an overabundance of zero observations.\n",
    "\n",
    "The model has two parameters, $\\pi$, the proportion of excess zero observations, and $\\lambda$, the mean of the Poisson distribution.  We assume that observations from this model are generated as follows.  First, a weighted coin with probability $\\pi$ of landing on heads is flipped.  If the result is heads, the observation is zero.  If the result is tails, the observation is generated from a Poisson distribution with mean $\\lambda$.  Note that there are two ways for an observation to be zero under this model:\n",
    "\n",
    "1. the coin is heads, and\n",
    "2. the coin is tails, and the sample from the Poisson distribution is zero.\n",
    "\n",
    "If $X$ has a zero-inflated Poisson distribution with parameters $\\pi$ and $\\lambda$, its probability mass function is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(X = 0)\n",
    "    & = \\pi + (1 - \\pi)\\ e^{-\\lambda} \\\\\n",
    "P(X = x)\n",
    "    & = (1 - \\pi)\\ e^{-\\lambda}\\ \\frac{\\lambda^x}{x!} \\textrm{ for } x > 0.\n",
    "\\end{align*}$$\n",
    "\n",
    "In this post, we will use the parameter values $\\pi = 0.3$ and $\\lambda = 2$.  The probability mass function of the zero-inflated Poisson distribution is shown below, next to a normal Poisson distribution, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from matplotlib import  pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pmf = probabilty mass function (?)\n",
    "#def zip_pmf(x, pi=pi, lambda_=lambda_):\n",
    "#    if pi < 0 or pi > 1 or lambda_ <= 0:\n",
    "#        return np.zeros_like(x)\n",
    "#    else:\n",
    "#        return (x == 0) * pi + (1 - pi) * stats.poisson.pmf(x, lambda_)\n",
    "    \n",
    "\n",
    "\n",
    "def round1_likelihood(degrees, answers, mu, sigma2):\n",
    "# likelihood(R|mu, sigma) = \n",
    "    likelihood = 1\n",
    "    # model the product\n",
    "    for t in range(len(degrees)):\n",
    "        # do we want the loc=0 here?\n",
    "        likelihood = likelihood * stats.bernoulli.pmf(answers[t], phi(degrees[t], mu, sigma2))\n",
    "    #print('The likelihood of these values is', likelihood)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate 1,000 observations from the zero-inflated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to estimate $\\pi$ and $\\lambda$ by maximum likelihood.  To do so, we define a class that inherits from `statsmodels`' `GenericLikelihoodModel` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats, optimize\n",
    "from math import erf, sqrt, exp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi(x, mu, sigma2):\n",
    "    # rewrote it to take sigma2 and then take the square root - not sure this is numerically smart... try without\n",
    "    sigma = math.sqrt(sigma2)\n",
    "    distribution = stats.norm(loc = mu, scale = sigma)\n",
    "    return distribution.cdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BernoulliCumNorm(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog=None, **kwds):\n",
    "        if exog is None:\n",
    "            exog = np.zeros_like(endog)\n",
    "            \n",
    "        super(BernoulliCumNorm, self).__init__(endog, exog, **kwds)\n",
    "    \n",
    "    def nloglikeobs(self, params):\n",
    "        mu = params[0]\n",
    "        sigma2 = params[1]\n",
    "\n",
    "        return -np.log(round1_likelihood(self.endog, self.exog, mu=mu, sigma2=sigma2))\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params is None:\n",
    "            # maybe witch endog & exog like done here but probably not (?) \n",
    "            #lambda_start = self.endog.mean()\n",
    "            mu_start = self.exog.mean()\n",
    "            sigma2_start = 2\n",
    "            \n",
    "            start_params = np.array([mu_start, sigma2_start])\n",
    "            \n",
    "        return super(BernoulliCumNorm, self).fit(start_params=start_params,\n",
    "                                                    maxiter=maxiter, maxfun=maxfun, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant1.p\n",
      "[[-2.25  4.5   2.25 -6.75  6.75 -4.5   2.25  6.75 -2.25 -6.75  4.5  -4.5\n",
      "   6.75  2.25  4.5  -2.25 -4.5  -6.75 -4.5   4.5   6.75 -2.25  2.25 -6.75\n",
      "  -6.75 -2.25 -4.5   4.5   2.25  6.75 -6.75  4.5   2.25  6.75 -2.25 -4.5\n",
      "   4.5  -6.75 -4.5   6.75  2.25 -2.25  4.5  -6.75 -2.25 -4.5   2.25  6.75\n",
      "   4.5  -6.75 -2.25 -4.5   2.25  6.75  4.5   6.75 -2.25  2.25 -4.5  -6.75]\n",
      " [ 0.    1.    0.    0.    1.    0.    1.    1.    0.    0.    1.    0.    1.\n",
      "   0.    1.    1.    0.    0.    0.    1.    1.    0.    1.    0.    0.    0.\n",
      "   0.    1.    1.    1.    0.    1.    1.    1.    0.    0.    1.    0.    0.\n",
      "   1.    1.    0.    1.    0.    0.    0.    1.    1.    1.    0.    0.    0.\n",
      "   1.    1.    1.    1.    0.    1.    0.    0.  ]]\n",
      "participant10.p\n",
      "[[ 0.   -6.75 -2.25  4.5   2.25 -4.5   6.75 -6.75  2.25  0.   -4.5   6.75\n",
      "   4.5  -2.25  0.   -6.75 -2.25  2.25  6.75 -4.5   4.5  -4.5   2.25  6.75\n",
      "   4.5   0.   -6.75 -2.25 -6.75  2.25  6.75 -4.5   0.    4.5  -2.25 -4.5\n",
      "   0.    6.75 -6.75  2.25 -2.25  4.5   2.25  4.5  -6.75  0.   -2.25 -4.5\n",
      "   6.75 -2.25  6.75  0.   -4.5   4.5   2.25 -6.75 -2.25  4.5   6.75 -6.75\n",
      "   2.25 -4.5   0.    2.25  6.75  0.   -2.25  4.5  -4.5  -6.75]\n",
      " [ 0.    0.    0.    1.    1.    0.    1.    0.    1.    0.    0.    1.    1.\n",
      "   0.    0.    0.    0.    1.    1.    0.    1.    0.    1.    1.    1.    0.\n",
      "   0.    0.    0.    0.    1.    0.    0.    1.    0.    0.    1.    1.    0.\n",
      "   0.    0.    1.    1.    1.    0.    0.    0.    0.    1.    0.    1.    0.\n",
      "   0.    1.    0.    1.    0.    1.    1.    0.    1.    0.    0.    1.    1.\n",
      "   0.    0.    1.    0.    0.  ]]\n",
      "participant11.p\n",
      "[[-2.25 -4.5   2.25 -6.75  6.75  4.5   0.    4.5   6.75 -4.5   0.   -6.75\n",
      "  -2.25  2.25  0.   -4.5  -2.25  6.75  2.25 -6.75  4.5   4.5  -6.75 -4.5\n",
      "   2.25  6.75  0.   -2.25  4.5   0.    6.75 -2.25 -4.5  -6.75  2.25  4.5\n",
      "  -2.25 -4.5   0.   -6.75  6.75  2.25 -6.75  6.75  2.25  4.5  -4.5   0.\n",
      "  -2.25  0.   -4.5  -6.75  2.25 -2.25  4.5   6.75  6.75  0.   -4.5  -6.75\n",
      "   2.25  4.5  -2.25 -6.75  2.25 -2.25  6.75  4.5  -4.5   0.  ]\n",
      " [ 0.    0.    0.    0.    1.    0.    0.    1.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    1.    1.    0.    1.    1.    0.    0.    0.    1.\n",
      "   0.    0.    1.    0.    1.    0.    0.    0.    0.    1.    0.    0.    0.\n",
      "   0.    1.    0.    0.    1.    0.    1.    0.    0.    0.    0.    0.    0.\n",
      "   1.    0.    1.    1.    1.    0.    0.    0.    1.    1.    0.    0.    0.\n",
      "   0.    1.    1.    0.    0.  ]]\n",
      "participant12.p\n",
      "[[ 2.25 -2.25  4.5  -6.75  6.75 -4.5   0.    0.    2.25 -6.75  6.75 -2.25\n",
      "   4.5  -4.5   2.25  0.   -4.5   4.5  -2.25  6.75 -6.75  6.75 -6.75  4.5\n",
      "   0.    2.25 -4.5  -2.25 -4.5   2.25 -2.25  0.   -6.75  6.75  4.5  -6.75\n",
      "   0.    6.75  4.5  -2.25 -4.5   2.25 -6.75 -2.25  6.75  4.5  -4.5   2.25\n",
      "   0.   -2.25  2.25  6.75  4.5  -4.5   0.   -6.75 -2.25 -4.5  -6.75  4.5\n",
      "   0.    2.25  6.75 -2.25  4.5   6.75 -6.75 -4.5   2.25  0.  ]\n",
      " [ 1.    0.    1.    0.    1.    0.    0.    0.    1.    0.    1.    0.    1.\n",
      "   0.    1.    0.    0.    1.    1.    1.    0.    1.    0.    1.    0.    1.\n",
      "   0.    0.    0.    1.    0.    0.    0.    1.    1.    0.    0.    1.    1.\n",
      "   0.    0.    0.    0.    0.    1.    1.    0.    0.    0.    0.    0.    1.\n",
      "   1.    0.    0.    0.    0.    0.    0.    1.    0.    0.    1.    0.    1.\n",
      "   1.    0.    0.    1.    0.  ]]\n",
      "participant13.p\n",
      "[[ 4.5  -4.5   0.   -2.25 -6.75  2.25  6.75  4.5   6.75 -2.25 -6.75 -4.5\n",
      "   0.    2.25 -4.5   4.5  -2.25 -6.75  0.    6.75  2.25  0.   -4.5  -2.25\n",
      "  -6.75  2.25  4.5   6.75 -6.75 -4.5  -2.25  6.75  4.5   2.25  0.   -2.25\n",
      "   4.5  -4.5   0.    2.25 -6.75  6.75 -2.25  6.75  0.   -4.5   4.5  -6.75\n",
      "   2.25  0.   -4.5   2.25  6.75  4.5  -2.25 -6.75 -2.25  2.25 -6.75 -4.5\n",
      "   0.    4.5   6.75 -2.25 -6.75  0.    4.5  -4.5   6.75  2.25]\n",
      " [ 1.    0.    0.    0.    0.    1.    1.    1.    1.    0.    0.    0.    0.\n",
      "   1.    0.    1.    0.    0.    0.    1.    0.    0.    0.    0.    0.    1.\n",
      "   1.    1.    0.    0.    0.    1.    1.    0.    0.    0.    1.    0.    0.\n",
      "   1.    0.    1.    0.    1.    0.    0.    1.    0.    1.    0.    0.    1.\n",
      "   1.    1.    0.    0.    0.    1.    0.    0.    0.    1.    1.    0.    0.\n",
      "   0.    1.    0.    1.    0.  ]]\n",
      "participant14.p\n",
      "[[-6.75 -2.25  4.5   2.25  6.75  0.   -4.5   0.   -6.75  6.75  4.5  -2.25\n",
      "  -4.5   2.25 -2.25  0.   -4.5   6.75  2.25  4.5  -6.75  0.    2.25 -2.25\n",
      "   6.75 -4.5  -6.75  4.5   0.    4.5  -2.25 -6.75  6.75  2.25 -4.5   0.\n",
      "  -6.75 -4.5   6.75 -2.25  4.5   2.25  6.75 -6.75 -2.25  2.25 -4.5   0.\n",
      "   4.5   0.    4.5  -4.5   2.25 -2.25  6.75 -6.75  6.75 -4.5  -6.75  4.5\n",
      "   2.25 -2.25  0.   -4.5   6.75 -6.75 -2.25  4.5   2.25  0.  ]\n",
      " [ 0.    0.    1.    1.    1.    0.    0.    0.    0.    1.    1.    0.    0.\n",
      "   1.    0.    0.    0.    1.    1.    1.    0.    0.    1.    0.    1.    0.\n",
      "   0.    1.    0.    1.    0.    0.    1.    1.    0.    0.    0.    0.    1.\n",
      "   0.    1.    1.    1.    0.    0.    1.    0.    0.    1.    0.    1.    0.\n",
      "   1.    0.    1.    0.    1.    0.    0.    1.    1.    0.    0.    0.    1.\n",
      "   0.    0.    1.    1.    0.  ]]\n",
      "participant15.p\n",
      "[[-4.5   4.5   6.75 -2.25  2.25 -6.75  0.    4.5  -6.75  0.   -4.5   2.25\n",
      "  -2.25  6.75  6.75 -2.25 -4.5   0.    4.5  -6.75  2.25 -2.25  2.25  6.75\n",
      "  -4.5   4.5  -6.75  0.   -2.25  0.    4.5  -6.75  2.25 -4.5   6.75  6.75\n",
      "   2.25 -4.5  -6.75  0.    4.5  -2.25  6.75 -6.75  0.   -4.5   2.25  4.5\n",
      "  -2.25 -2.25 -6.75 -4.5   2.25  4.5   6.75  0.   -2.25  2.25 -4.5   0.\n",
      "  -6.75  6.75  4.5  -6.75  6.75 -2.25  4.5   2.25  0.   -4.5 ]\n",
      " [ 0.    1.    1.    0.    1.    0.    0.    1.    0.    0.    0.    1.    0.\n",
      "   1.    1.    0.    0.    0.    1.    0.    1.    0.    1.    1.    0.    1.\n",
      "   0.    0.    0.    0.    1.    0.    1.    0.    1.    1.    1.    0.    0.\n",
      "   0.    1.    0.    1.    0.    1.    0.    1.    1.    0.    0.    0.    0.\n",
      "   1.    1.    1.    0.    0.    1.    0.    1.    0.    1.    1.    0.    1.\n",
      "   0.    1.    1.    1.    0.  ]]\n",
      "participant2.p\n",
      "[[-6.75  6.75  4.5   2.25 -2.25 -4.5   6.75  2.25 -4.5  -6.75 -2.25  4.5\n",
      "   2.25 -6.75  4.5  -2.25 -4.5   6.75 -2.25  6.75  2.25  4.5  -4.5  -6.75\n",
      "  -2.25  6.75  4.5  -6.75  2.25 -4.5  -2.25  2.25 -4.5   6.75  4.5  -6.75\n",
      "  -4.5  -2.25  4.5  -6.75  2.25  6.75  6.75 -4.5  -2.25  2.25 -6.75  4.5\n",
      "   2.25  6.75  4.5  -4.5  -2.25 -6.75  6.75 -6.75 -4.5  -2.25  4.5   2.25]\n",
      " [ 0.    1.    1.    0.    0.    0.    1.    1.    0.    0.    0.    1.    0.\n",
      "   0.    1.    0.    0.    1.    0.    1.    0.    1.    0.    0.    0.    1.\n",
      "   1.    0.    0.    0.    0.    1.    0.    1.    1.    0.    0.    0.    1.\n",
      "   0.    0.    1.    1.    0.    0.    1.    0.    1.    1.    1.    1.    0.\n",
      "   0.    0.    1.    0.    0.    0.    1.    0.  ]]\n",
      "participant3.p\n",
      "[[-6.75 -4.5   2.25 -2.25  6.75  4.5   6.75 -2.25  2.25 -6.75  4.5  -4.5\n",
      "  -6.75 -4.5   2.25  4.5   6.75 -2.25 -6.75  6.75  2.25  4.5  -4.5  -2.25\n",
      "   4.5  -2.25 -4.5  -6.75  2.25  6.75 -6.75  4.5  -4.5   6.75  2.25 -2.25\n",
      "   2.25 -6.75  4.5  -2.25  6.75 -4.5  -4.5  -6.75 -2.25  2.25  4.5   6.75\n",
      "   4.5   6.75 -6.75 -2.25  2.25 -4.5   4.5  -4.5   2.25 -2.25 -6.75  6.75]\n",
      " [ 0.    0.    1.    0.    1.    1.    1.    0.    0.    0.    1.    0.    0.\n",
      "   0.    1.    1.    1.    0.    0.    1.    0.    1.    0.    0.    1.    0.\n",
      "   0.    0.    1.    1.    0.    1.    0.    1.    0.    0.    0.    0.    1.\n",
      "   0.    1.    0.    0.    0.    0.    0.    0.    1.    1.    1.    0.    0.\n",
      "   1.    0.    1.    0.    1.    0.    0.    1.  ]]\n",
      "participant4.p\n",
      "[[-6.75 -4.5   4.5   6.75  2.25 -2.25 -4.5  -2.25  4.5   6.75  2.25 -6.75\n",
      "   6.75  4.5  -6.75 -2.25 -4.5   2.25 -6.75  6.75  2.25 -2.25  4.5  -4.5\n",
      "   4.5  -6.75 -4.5  -2.25  6.75  2.25  4.5  -2.25  2.25 -4.5   6.75 -6.75\n",
      "  -4.5   6.75  2.25  4.5  -2.25 -6.75  6.75 -6.75  2.25  4.5  -4.5  -2.25\n",
      "  -4.5   2.25  6.75 -6.75  4.5  -2.25  4.5   2.25  6.75 -2.25 -4.5  -6.75]\n",
      " [ 0.    0.    1.    1.    1.    1.    0.    0.    1.    1.    1.    0.    1.\n",
      "   1.    0.    0.    0.    1.    0.    1.    1.    0.    1.    0.    1.    0.\n",
      "   0.    0.    1.    0.    1.    0.    0.    0.    1.    0.    0.    1.    1.\n",
      "   1.    0.    0.    1.    0.    0.    1.    0.    0.    0.    0.    1.    0.\n",
      "   1.    0.    1.    0.    1.    0.    0.    0.  ]]\n",
      "participant5.p\n",
      "[[-6.75  6.75 -2.25  4.5  -4.5   0.    2.25 -2.25  4.5  -6.75  6.75  0.\n",
      "  -4.5   2.25  2.25 -2.25  6.75 -6.75  4.5  -4.5   0.   -6.75  0.    2.25\n",
      "   4.5   6.75 -2.25 -4.5   2.25  0.   -2.25  6.75  4.5  -6.75 -4.5  -2.25\n",
      "   6.75  2.25  0.    4.5  -4.5  -6.75 -4.5   0.    2.25 -2.25 -6.75  4.5\n",
      "   6.75 -2.25 -4.5   6.75  4.5   0.    2.25 -6.75  6.75  4.5  -6.75  2.25\n",
      "  -4.5   0.   -2.25 -6.75  0.    4.5   2.25 -2.25  6.75 -4.5 ]\n",
      " [ 0.    1.    0.    1.    0.    1.    1.    0.    1.    0.    1.    0.    0.\n",
      "   1.    1.    0.    1.    0.    1.    0.    0.    0.    1.    1.    1.    1.\n",
      "   0.    0.    1.    0.    0.    1.    1.    0.    0.    0.    1.    0.    0.\n",
      "   1.    0.    0.    0.    0.    1.    0.    0.    1.    1.    0.    0.    1.\n",
      "   1.    0.    1.    0.    1.    1.    0.    1.    1.    0.    0.    0.    0.\n",
      "   1.    0.    0.    1.    0.  ]]\n",
      "participant6.p\n",
      "[[ 2.25  6.75 -4.5  -2.25  4.5  -6.75  0.   -6.75  6.75 -4.5  -2.25  2.25\n",
      "   4.5   0.   -4.5   4.5   0.   -6.75  2.25  6.75 -2.25  6.75  0.   -2.25\n",
      "  -4.5  -6.75  4.5   2.25  6.75  4.5  -4.5  -2.25  0.   -6.75  2.25 -4.5\n",
      "   2.25  4.5  -6.75 -2.25  6.75  0.   -6.75  6.75  2.25  0.   -4.5  -2.25\n",
      "   4.5   6.75  2.25 -6.75 -2.25  4.5  -4.5   0.   -4.5  -6.75  4.5   2.25\n",
      "   0.    6.75 -2.25 -6.75  6.75  0.   -2.25  2.25  4.5  -4.5 ]\n",
      " [ 0.    1.    0.    0.    1.    0.    0.    0.    1.    0.    0.    1.    1.\n",
      "   0.    0.    1.    0.    0.    1.    1.    0.    1.    0.    0.    0.    0.\n",
      "   1.    1.    1.    1.    0.    0.    0.    0.    1.    0.    1.    1.    0.\n",
      "   0.    1.    0.    0.    1.    1.    0.    0.    0.    1.    1.    0.    0.\n",
      "   0.    1.    0.    1.    0.    0.    1.    1.    0.    1.    0.    0.    1.\n",
      "   1.    0.    1.    1.    0.  ]]\n",
      "participant7.p\n",
      "[[-4.5   6.75 -6.75  4.5   2.25 -2.25  0.    0.   -2.25  2.25  6.75 -4.5\n",
      "  -6.75  4.5  -6.75 -2.25 -4.5   2.25  0.    4.5   6.75  4.5   2.25  6.75\n",
      "  -2.25 -6.75 -4.5   0.   -6.75 -4.5  -2.25  6.75  2.25  4.5   0.   -2.25\n",
      "   6.75  2.25  4.5  -6.75 -4.5   0.   -4.5   2.25 -6.75 -2.25  6.75  4.5\n",
      "   0.    6.75  0.   -4.5   4.5  -2.25 -6.75  2.25  2.25 -2.25 -4.5   4.5\n",
      "   0.    6.75 -6.75 -2.25 -6.75  4.5   2.25 -4.5   0.    6.75]\n",
      " [ 0.    1.    0.    1.    1.    0.    0.    0.    0.    1.    1.    0.    0.\n",
      "   1.    0.    0.    0.    1.    0.    1.    1.    1.    1.    1.    0.    0.\n",
      "   0.    0.    0.    0.    0.    1.    1.    1.    0.    0.    1.    1.    1.\n",
      "   0.    0.    0.    0.    1.    0.    0.    1.    1.    0.    1.    0.    0.\n",
      "   1.    0.    0.    1.    1.    0.    0.    1.    0.    1.    0.    0.    0.\n",
      "   1.    0.    0.    1.    1.  ]]\n",
      "participant8.p\n",
      "[[ 2.25  4.5   0.   -6.75 -4.5  -2.25  6.75  6.75 -6.75 -4.5   0.    4.5\n",
      "  -2.25  2.25 -6.75  6.75 -2.25  2.25  4.5  -4.5   0.   -6.75 -2.25 -4.5\n",
      "   4.5   2.25  6.75  0.    2.25  0.    4.5  -4.5  -2.25  6.75 -6.75 -6.75\n",
      "   2.25  6.75  0.   -2.25 -4.5   4.5   4.5   6.75  0.   -6.75 -4.5  -2.25\n",
      "   2.25  0.    2.25  6.75 -2.25 -4.5   4.5  -6.75 -2.25  2.25 -6.75  0.\n",
      "   6.75 -4.5   4.5  -4.5  -6.75  0.   -2.25  4.5   2.25  6.75]\n",
      " [ 1.    1.    0.    0.    0.    0.    1.    1.    0.    0.    0.    1.    1.\n",
      "   0.    0.    1.    0.    0.    1.    0.    0.    0.    0.    0.    1.    0.\n",
      "   1.    0.    0.    0.    1.    0.    0.    1.    0.    0.    1.    1.    0.\n",
      "   0.    0.    1.    0.    1.    0.    0.    0.    0.    1.    0.    1.    1.\n",
      "   0.    0.    1.    0.    0.    0.    0.    0.    1.    0.    1.    0.    0.\n",
      "   0.    0.    1.    1.    1.  ]]\n",
      "participant9.p\n",
      "[[ 2.25 -4.5   6.75 -6.75 -2.25  4.5   0.   -2.25 -6.75  4.5   0.   -4.5\n",
      "   6.75  2.25 -6.75  0.    2.25 -2.25  4.5  -4.5   6.75 -6.75  4.5   6.75\n",
      "   0.   -2.25 -4.5   2.25 -4.5  -6.75  2.25  0.    4.5  -2.25  6.75  2.25\n",
      "   4.5   0.    6.75 -4.5  -6.75 -2.25  0.   -2.25  2.25 -6.75  4.5   6.75\n",
      "  -4.5   0.    6.75  2.25 -2.25 -4.5   4.5  -6.75 -4.5  -2.25  0.    2.25\n",
      "   6.75  4.5  -6.75  0.   -2.25  4.5  -4.5   6.75  2.25 -6.75]\n",
      " [ 1.    0.    1.    0.    0.    1.    0.    0.    0.    1.    0.    0.    1.\n",
      "   1.    0.    1.    1.    0.    1.    0.    1.    0.    1.    1.    0.    0.\n",
      "   0.    1.    0.    0.    1.    0.    1.    0.    1.    1.    1.    0.    1.\n",
      "   0.    0.    0.    0.    0.    1.    0.    1.    1.    0.    0.    1.    1.\n",
      "   0.    0.    1.    0.    0.    0.    0.    1.    1.    1.    0.    0.    0.\n",
      "   1.    0.    1.    1.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir('data/preprocessed/audio'):\n",
    "    if filename.endswith('.p'):\n",
    "        print(filename)\n",
    "        data = pickle.load( open('data/preprocessed/audio/' + filename, \"rb\" ) )\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key component of this class is the method `nloglikeobs`, which returns the negative log likelihood of each observed value in `endog`.  Secondarily, we must also supply reasonable initial guesses of the parameters in `fit`.  Obtaining the maximum likelihood estimate is now simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.25, -4.5 ,  6.75, -6.75, -2.25,  4.5 ,  0.  , -2.25, -6.75,\n",
       "        4.5 ,  0.  , -4.5 ,  6.75,  2.25, -6.75,  0.  ,  2.25, -2.25,\n",
       "        4.5 , -4.5 ,  6.75, -6.75,  4.5 ,  6.75,  0.  , -2.25, -4.5 ,\n",
       "        2.25, -4.5 , -6.75,  2.25,  0.  ,  4.5 , -2.25,  6.75,  2.25,\n",
       "        4.5 ,  0.  ,  6.75, -4.5 , -6.75, -2.25,  0.  , -2.25,  2.25,\n",
       "       -6.75,  4.5 ,  6.75, -4.5 ,  0.  ,  6.75,  2.25, -2.25, -4.5 ,\n",
       "        4.5 , -6.75, -4.5 , -2.25,  0.  ,  2.25,  6.75,  4.5 , -6.75,\n",
       "        0.  , -2.25,  4.5 , -4.5 ,  6.75,  2.25, -6.75])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/polaschwoebel/anaconda/lib/python3.5/site-packages/statsmodels/base/model.py:466: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliCumNorm(data[1,:], data[0,:])\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'converged': False,\n",
       " 'fcalls': 3999,\n",
       " 'fopt': inf,\n",
       " 'iterations': 1000,\n",
       " 'warnflag': 2}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mle_retvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have estimated the parameters fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many advantages to buying into the `statsmodels` ecosystem and subclassing `GenericLikelihoodModel`.  The already-written `statsmodels` code handles storing the observations and the interaction with `scipy.optimize` for us.  (It is possible to control the use of `scipy.optimize` through keyword arguments to `fit`.)\n",
    "\n",
    "We also gain access to many of `statsmodels`' built in model analysis tools.  For example, we can use bootstrap resampling to estimate the variation in our parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.ZeroInflatedPoisson'>\n"
     ]
    }
   ],
   "source": [
    "boot_mean, boot_std, boot_samples = results.bootstrap(nrep=500, store=True)\n",
    "boot_pis = boot_samples[:, 0]\n",
    "boot_lambdas = boot_samples[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6cAAAGSCAYAAAAfNLdHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf758dbb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (pi_ax, lambda_ax) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "\n",
    "sns.boxplot(boot_pis, ax=pi_ax, names=['$\\pi$'], color=palette[0]);\n",
    "sns.boxplot(boot_lambdas, ax=lambda_ax, names=['$\\lambda$'], color=palette[1]);\n",
    "\n",
    "fig.suptitle('Boostrap distribution of maximum likelihood estimates');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next time you are fitting a model using maximum likelihood, try integrating with `statsmodels` to take advantage of the significant amount of work that has gone into its ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
