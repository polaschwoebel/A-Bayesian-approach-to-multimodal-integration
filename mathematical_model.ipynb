{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main goal: \n",
    "Fit a model for multisensory localization as a weighted average of visual and auditory information in a first round of experiments, then test it in a second round of experiment. The underlying assumption is that the mulitmodel localization can be modelled as some weighted average of the purely auditory/visual inputs via:\n",
    "\n",
    "$$ L^* = w_v L^*_v +  w_a L^*_a  $$\n",
    "\n",
    "where \n",
    "$w_v, w_a$  are the weights for the auditory/visual input, \n",
    "$L^*_w, L^*_a$ are the location estimates based on the purely visual/auditory input and \n",
    "$L^*$ is the location estimate based on the combined input ('dependant variable'? something that we want to find out here - even though the weights are the final goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First round of experiments: purely visual & purely auditory\n",
    "__Goals__:\n",
    "- estimate weights $w_a, w_v$ for the auditory and visual information in the model of the multisensory setup\n",
    "- find location estimates $L_a^*, L_v^*$ based on the purely auditory/visual inputs \n",
    "\n",
    "__Method__:\n",
    "This will be achieved via  fitting a cumulative normal distribution to fit the datapoints we see - using the method of *Maximum Likelihood Estimation* (MLE). Recall that a normal distribution is defined by it's variance $\\sigma^2$ and it's mean $\\mu$. We thus want to estimate \n",
    "\n",
    "$$ \\widehat{\\sigma}^2, \\widehat{\\mu} = argmax_{\\mu, \\sigma} \\prod_{t=1}^{T} p_t^{r_t} (1-p_t)^{1-r_t}\n",
    "= argmax_{\\mu, \\sigma} \\prod_{t=1}^{T} (\\frac{1}{2}(1+erf(\\frac{x-\\mu}{\\sqrt(2\\sigma^2)})) )^{r_t} (1-(1+erf(\\frac{x-\\mu}{\\sqrt(2\\sigma^2)})))^{1-r_t}\n",
    "$$\n",
    "\n",
    "*TODO: annoying work of explaining all the variables and why a Bernoulli probability is the right choice*\n",
    "\n",
    "Battaglia et al. suggested the following modification to the \"classical MLE\" model: transform it into a Bayesian model by adding priors $p(\\sigma)$ and $p(\\mu)$. The reason is the (evolutionary?) bias towards visual input as a more reliable source that the MLE model doesn't capture. In fact, some authors claim that this visual dominance is even so strong that the visual sensory input will completely dominate the auditory one (*visual capture theroy*). We thus refine our model into \n",
    "\n",
    "$$ \\widehat{\\sigma}^2, \\widehat{\\mu} = argmax_{\\mu, \\sigma} \\prod_{t=1}^{T} (\\frac{1}{2}(1-erf(\\frac{x-\\mu}{\\sqrt(2\\sigma^2)})) )^{r_t} (1-(1-erf(\\frac{x-\\mu}{\\sqrt(2\\sigma^2)})))^{1-r_t} \\cdot p_{\\sigma^2}(\\sigma^2) \\cdot p_{\\mu}(\\mu)$$\n",
    "\n",
    "The prior for $\\mu$ is just a uniform distribution. More interestingly, the prior for $\\sigma^2$ is an *inverse gamma distribution* that we model in a way that it favors small variances (corresponding to __reliable__ sensory input) \n",
    "[*TODO: explain why and how this is, write out mathzzz*]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed python functionalities:\n",
    "    - argmax / maximum likelihood\n",
    "    - inverse gamma distribution\n",
    "    - uniform distribution\n",
    "    \n",
    "possibly useful:\n",
    "    - stats.bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats, optimize\n",
    "from math import erf, sqrt, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r should be a list of {0,1} with the outcomes\n",
    "# phi a cumulative normal distribution \n",
    "\n",
    "def phi(x, mu, sigma_sq):\n",
    "    #'Cumulative distribution function for the standard normal distribution'\n",
    "    # -> are we sure the x don't refer to the visual angles instead? YES I THINK SO\n",
    "    return (1.0 + erf(x - mu / sqrt(2.0*sigma_sq))) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def round1_likelihood(r, mu, sigma_sq):\n",
    "# likelihood(R|mu, sigma) = \n",
    "    likelihood = 1\n",
    "    # model the product\n",
    "    for t in range(len(r)):\n",
    "        likelihood = likelihood * stats.bernoulli.pmf(r[1][t], phi(r[0][t], mu, sigma_sq), loc=0) \n",
    "        # TODO: *p(mu)*p(sigma_sq) - i.e. go from MLE to Bayesian approach\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a mockup example for now. I THINK THERE IS AN ERROR IN THEIR MODEL DESCRIPTION! \n",
    "# you need to take the positional information into account, too! \n",
    "# so r consists of two pieces of information: degree and angle\n",
    "degrees = [-4.5, -3. -1.5, 0 , 1.5, 3, 4.5]\n",
    "answers = [0, 0, 0, 0, 1, 1, 1]\n",
    "r = (degrees, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round1_argmin(likelihood):\n",
    "    result = optimize.minimize(lambda x: 1-likelihood(r, *x), (0,2))['x']\n",
    "    mu = result[0]\n",
    "    sigma2 = result[1]\n",
    "    return mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 2.0\n"
     ]
    }
   ],
   "source": [
    "mu, sigma2 = round1_argmin(round1_likelihood)\n",
    "print(mu, sigma2)\n",
    "# YAY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the weights from the sigmas we calculated\n",
    "def get_weight_from_variance(variance, other_variance):\n",
    "    return (1/variance)/((1/variance + 1/other_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second round of experiments:\n",
    "Compare the weights obtained in the second experiment (and thus the model for multisensory integration) and compare it with empirical results. The empirical weights are again found via a maximum likelihood estimation similar to the monosensory trials, but with a modified probability $p_t$:\n",
    "\\begin{align}\n",
    "\\widehat{w}_a, \\widehat{w}_v &= argmax_{w_a, w_v} \\prod_{t=1}^{T} p_t^{r_t} (1-p_t)^{1-r_t} \\\\\n",
    "&= argmax_{w_a, w_v} \\prod_{t=1}^{T} (\\frac{1}{1 + exp[-(L_c - L_s)/\\tau]})^{r_t} (1-(\\frac{1}{1 + exp[-(L_c - L_s)/\\tau])}^{1-r_t}) \\\\\n",
    "&= argmax_{w_a, w_v} \\prod_{t=1}^{T} (\\frac{1}{1 + exp[-(w_vL_v^c + w_aL_a^c - (w_vL_v^s + w_aL_a^s))/\\tau]})^{r_t} (1-(\\frac{1}{1 + exp[-(w_vL_v^c + w_aL_a^c - (w_vL_v^s + w_aL_a^s)))/\\tau])}^{1-r_t})\n",
    "\\end{align}\n",
    "\n",
    "*NOTE: I am still ab bit confused about their explanation with 'location estimates' here. I think we probably just have to use the actual locations because we don't really have any location estimates other than the mean of the two distributions fitted in the first round which should just be very close to zero. Is this just a mistake in the paper?\n",
    "Related question: the visual and audio location is the same in the comparison stimulus then, right? Then L_c collapses  (since the weights sum to 1) Also it makes no sense to optimize for both of the weights since they are constrained to sum to one.*\n",
    "With these assumptions the equation would simplify to \n",
    "\n",
    "$$\n",
    "argmax_{w_a, w_v} \\prod_{t=1}^{T} (\\frac{1}{1 + exp[-(L_c - (w_vL_v^s + (1-w_v)L_a^s))/\\tau]})^{r_t} (1-(\\frac{1}{1 + exp[-(L_c - (w_vL_v^s + (1-w_v)L_a^s)))/\\tau])}^{1-r_t}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def round2_likelihood(r, w_a, w_v, tau):\n",
    "# rewrite w_a = 1 - w_v\n",
    "def round2_likelihood(r, w_v, tau):\n",
    "    # remember r is a tuple of (location, answer)\n",
    "    L = r[0]\n",
    "    answers = r[1]\n",
    "    # constants\n",
    "    l_s_a = 1.5\n",
    "    l_s_v = -1.5\n",
    "    likelihood = 1\n",
    "    # model the product\n",
    "    for t in range(len(L)):\n",
    "        #likelihood = (likelihood * 1 / (1 + exp(- (L[t] - w_v*l_s_v + w_a*l_s_a))/tau)**answers[t] \n",
    "        #                        * ( 1 - 1 / (1 + exp(- (L[t] - w_v*l_s_v + w_a*l_s_a))/tau))**(1 - answers[t]))\n",
    "        # rewrite w_a = 1 - w_v\n",
    "        likelihood = likelihood * (1 / (1 + exp(- (L[t] - (w_v*l_s_v + (1-w_v)*l_s_a))/tau))**answers[t] \n",
    "                                * ( 1 - (1 / (1 + exp(- (L[t] - w_v*l_s_v + (1-w_v)*l_s_a))/tau)))**(1 - answers[t]))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def round2_argmin(likelihood):\n",
    "    result = optimize.minimize(lambda x: 1-likelihood(r, *x), \n",
    "                               [0.2, 10], method = 'SLSQP', bounds = [(0.5, 1),(None, None)], tol =0.0000000000000001)['x']\n",
    "    w_a = result[0]\n",
    "    tau = result[1]\n",
    "    return w_a, tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new mockup which is skewed towards vision\n",
    "degrees = [-4.5, -3. -1.5, 0 , 1.5, 3, 4.5]\n",
    "answers = [0, 0, 0, 0, 1, 1, 1]\n",
    "r = (degrees, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50000000000117861, -20.085381579078952)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round2_argmin(round2_likelihood)\n",
    "# STRANGE: the value seems to not go under 0.5 (which it won't in the experiment anyways but mathematically that \n",
    "# should be possible for sure!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
